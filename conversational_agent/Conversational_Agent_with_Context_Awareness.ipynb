{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cMNGHRmLfFY",
        "outputId": "6c80078e-3f13-42f0-ce9b-20741c934bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.4 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/831.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.9/831.9 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-openai 0.3.28 requires openai<2.0.0,>=1.86.0, but you have openai 0.27.10 which is incompatible.\n",
            "langchain-openai 0.3.28 requires tiktoken<1,>=0.7, but you have tiktoken 0.3.3 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.37.0 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        " %pip install -q langchain langchain_experimental genai python-dotenv langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "dseXu8y4Llck"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load environment variables and initialize the language model"
      ],
      "metadata": {
        "id": "qu1z9EhXNu9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", max_tokens=1000, temperature=0)"
      ],
      "metadata": {
        "id": "dbOM8x3ZN0gf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a simple in-memory store for chat histories"
      ],
      "metadata": {
        "id": "AB5SaILTOQGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store = {}\n",
        "\n",
        "def get_chat_history(session_id: str):\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]"
      ],
      "metadata": {
        "id": "EQTGYySrOCzk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the prompt template"
      ],
      "metadata": {
        "id": "LPip89-IOyr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an intelligent and helpful AI assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "JL0HTnsXO0r4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine the prompt and model into a runnable chain"
      ],
      "metadata": {
        "id": "9AVtrs5oPczq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "G-ywyAC8PULj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrap the chain with message history"
      ],
      "metadata": {
        "id": "PsrHGynmQFqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_chat_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "Zip2nCViQHsn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use case"
      ],
      "metadata": {
        "id": "jNEDRaivQkcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = \"user_123\"\n",
        "\n",
        "\n",
        "response1 = chain_with_history.invoke(\n",
        "    {\"input\": \"Hello! ?\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "print(\"### AI Response 1:\\n\")\n",
        "print(response1.content)\n",
        "\n",
        "response2 = chain_with_history.invoke(\n",
        "    {\"input\": \"What was my previous message?\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "print(\"\\n### AI Response 2:\\n\")\n",
        "print(response2.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33iJH_H4QMwz",
        "outputId": "4c3c47a8-dde5-421f-df38-04ce382b09b4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### AI Response 1:\n",
            "\n",
            "Hello! How can I help you today?\n",
            "\n",
            "### AI Response 2:\n",
            "\n",
            "Your previous message was \"Hello! ?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n### Conversation History:\\n\")\n",
        "for message in store[session_id].messages:\n",
        "    print(f\"*{message.type}:*{message.content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAvPoA-VQtw5",
        "outputId": "09af8b66-4683-4a2a-c041-51de536c56bc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Conversation History:\n",
            "\n",
            "*human:*Hello! ?\n",
            "\n",
            "*ai:*Hello! How can I help you today?\n",
            "\n",
            "*human:*What was my previous message?\n",
            "\n",
            "*ai:*As a large language model, I have no memory of past conversations. Therefore, I don't know what your previous message was.\n",
            "\n",
            "*human:*Hello! ?\n",
            "\n",
            "*ai:*Hello! How can I help you today?\n",
            "\n",
            "*human:*What was my previous message?\n",
            "\n",
            "*ai:*Your previous message was \"Hello! ?\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = \"user_1234\"\n",
        "\n",
        "\n",
        "response1 = chain_with_history.invoke(\n",
        "    {\"input\": \"Hello! who are you?\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "print(\"### AI Response 1:\\n\")\n",
        "print(response1.content)\n",
        "\n",
        "response2 = chain_with_history.invoke(\n",
        "    {\"input\": \"What was my previous message?\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "print(\"\\n### AI Response 2:\\n\")\n",
        "print(response2.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7sbc8R3Q196",
        "outputId": "71141257-b839-47e4-cc98-f3ea692e553a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### AI Response 1:\n",
            "\n",
            "I am a large language model, trained by Google.\n",
            "\n",
            "### AI Response 2:\n",
            "\n",
            "Your previous message was \"Hello! who are you?\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n### Conversation History:\\n\")\n",
        "for message in store[session_id].messages:\n",
        "    print(f\"*{message.type}:*{message.content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUXx407DRPeV",
        "outputId": "a323c151-9445-4488-df33-fae7a795927c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Conversation History:\n",
            "\n",
            "*human:*Hello! who are you?\n",
            "\n",
            "*ai:*I am a large language model, trained by Google.\n",
            "\n",
            "*human:*What was my previous message?\n",
            "\n",
            "*ai:*Your previous message was \"Hello! who are you?\".\n",
            "\n",
            "*human:*Hello! who are you?\n",
            "\n",
            "*ai:*I am a large language model, trained by Google.\n",
            "\n",
            "*human:*What was my previous message?\n",
            "\n",
            "*ai:*Your previous message was \"Hello! who are you?\".\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = \"user_12345\"\n",
        "\n",
        "\n",
        "response1 = chain_with_history.invoke(\n",
        "    {\"input\": \"Hello! What's the highest mountain in the world and where is it located?\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "print(\"### AI Response 1:\\n\")\n",
        "print(response1.content)\n",
        "\n",
        "response2 = chain_with_history.invoke(\n",
        "    {\"input\": \"What was my previous message?\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "print(\"\\n### AI Response 2:\\n\")\n",
        "print(response2.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zgqSNKURYCP",
        "outputId": "623b25ba-9063-41e1-f40c-640f2d1d19bc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### AI Response 1:\n",
            "\n",
            "Hello! As I mentioned before, the highest mountain in the world is **Mount Everest**, located in the **Himalayas** on the border between **Nepal** and **China (Tibet Autonomous Region)**.\n",
            "\n",
            "### AI Response 2:\n",
            "\n",
            "Your previous message was: \"Hello! What's the highest mountain in the world and where is it located?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = \"user_12345\"\n",
        "\n",
        "\n",
        "response1 = chain_with_history.invoke(\n",
        "    {\"input\": \"Hello!? Can you give me a brief history about Mount Everest\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "print(\"### AI Response 1:\\n\")\n",
        "print(response1.content)\n",
        "\n",
        "response2 = chain_with_history.invoke(\n",
        "    {\"input\": \"What was my previous message?\"},\n",
        "    config={\"configurable\": {\"session_id\": session_id}}\n",
        ")\n",
        "print(\"\\n### AI Response 2:\\n\")\n",
        "response2.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "QTqTwJHTR15W",
        "outputId": "2d6b84df-b704-467c-e4b4-78dd93ce8fdd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### AI Response 1:\n",
            "\n",
            "Hello! I can certainly give you a brief history of Mount Everest again. Here it is:\n",
            "\n",
            "*   **Early History & Naming:** Known locally as Chomolungma (Tibetan) and Sagarmatha (Nepali). In 1856, the Great Trigonometrical Survey of India identified it as the highest peak and named it after Sir George Everest.\n",
            "\n",
            "*   **Early Attempts:** Began in the 1920s, primarily by British expeditions from Tibet. George Mallory and Andrew Irvine disappeared in 1924.\n",
            "\n",
            "*   **First Ascent:** May 29, 1953, Sir Edmund Hillary and Tenzing Norgay reached the summit as part of a British expedition.\n",
            "\n",
            "*   **Later Ascents & Routes:** New routes were established, and the South Col route (via Nepal) became the most popular.\n",
            "\n",
            "*   **Commercialization & Challenges:** Increased commercialization has led to overcrowding, environmental concerns, and safety issues.\n",
            "\n",
            "*   **Modern Era:** Hundreds attempt to summit each year. Challenges remain related to climate change, waste, and tourism.\n",
            "\n",
            "### AI Response 2:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your previous message was: \"Hello!? Can you give me a brief history about Mount Everest\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNF3GX7vSU7F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}